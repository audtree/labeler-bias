{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements:\n",
    "\n",
    "- https://selenium-python.readthedocs.io/locating-elements.html\n",
    "- https://sites.google.com/a/chromium.org/chromedriver/downloads\n",
    "\n",
    "Instead of looping through pages, uses \"All Results\" button under \"Export Citations\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from enum import Enum\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException, ElementNotInteractableException\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "\n",
    "from pybtex.database.input import bibtex\n",
    "import pybtex.errors\n",
    "pybtex.errors.set_strict_mode(False)\n",
    "\n",
    "# from urllib.request import urlopen, Request\n",
    "# from selenium.webdriver.common.keys import Keys\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "# import bibtexparser\n",
    "# from html.parser import HTMLParser\n",
    "# from itertools import permutations \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0: No Screenshots\n",
    "# 1: One Screenshot for each query (recommended)\n",
    "# 2: Screenshots of different steps to find out why crawler might not work\n",
    "DEBUG = 2\n",
    "\n",
    "acm_maxpage = 39\n",
    "\n",
    "GLOBAL_ERROR_LIST = []\n",
    "urls = []\n",
    "\n",
    "# chrome_options = Options()\n",
    "# chrome_options.add_argument(\"--headless=new\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings for crawling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchWhere(Enum):\n",
    "        Title = 1\n",
    "        Abstract = 2\n",
    "        TitleAbstract = 3 #Keywords have to be in Title OR Abstract\n",
    "        Anywhere = 4\n",
    "        Fulltext = 5\n",
    "class Library(Enum):\n",
    "        IEEE = 1\n",
    "        ACM = 2\n",
    "        ScienceDirect = 3\n",
    "\n",
    "year_min = 2015 # Set to earliest year which should be crawled\n",
    "year_max = 2024 # Set to latest year whichh should be crawled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup for crawler\n",
    "\n",
    "### function to crawl: crawl(keywords, LIBRARY, titlesearch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define read and write permissions (in octal notation)\n",
    "directory_path = os.getcwd()+ '/acm-query-dl'\n",
    "new_permissions = 644  # read and write\n",
    "os.chmod(directory_path, new_permissions)\n",
    "\n",
    "def setupCrawler(dl_folder):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('window-size=1920,1080')\n",
    "    dl = os.getcwd() + '/acm-query-dl'\n",
    "    p = {\"download.default_directory\": dl}\n",
    "    options.add_experimental_option(\"prefs\", p)\n",
    "    op = webdriver.ChromeOptions()\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    print(\"Driver setup complete.\")\n",
    "    return driver\n",
    "\n",
    "def crawl(keywords_list, library, searchWhere):\n",
    "    print(f\"Start crawling {library}\")\n",
    "    if library == Library.ACM:\n",
    "        keywords = [[item.replace(\" \", \"+\") for item in keywords] for keywords in keywords_list]\n",
    "        saveACMBib(keywords, Library.ACM, searchWhere)\n",
    "    else:\n",
    "        print(f\"Library {library} not yet supported\")\n",
    "\n",
    "def getURL(keywords, library, searchWhere, concatentation=\"AND\"):\n",
    "    URL = \"\"\n",
    "    search = \"\"\n",
    "    if library == Library.ACM:\n",
    "        titleSearch = \"doSearch?AllField=\"\n",
    "        for i, keyword in enumerate(keywords):\n",
    "            search += f\"%22{keyword}%22\"\n",
    "            if (i < len(keywords)-1):\n",
    "                search += f\"+{concatentation}+\"\n",
    "        match searchWhere:\n",
    "            case SearchWhere.Title:\n",
    "                print(\"Searching ACM for title only\")\n",
    "                titleSearch = f\"doSearch?fillQuickSearch=false&expand=dl&field1=Title&text1={search}\"\n",
    "            case SearchWhere.Abstract:\n",
    "                print(\"Searching ACM for abstract only\")\n",
    "                titleSearch = f\"doSearch?fillQuickSearch=false&expand=dl&field1=Abstract&text1={search}\"\n",
    "            case SearchWhere.TitleAbstract:\n",
    "                print(\"ACM does not support searching for keywords in Title OR Abstract. Please use Title and Abstract search seperately.\")\n",
    "            case SearchWhere.Anywhere:\n",
    "                print(\"Searching ACM for anywhere\")\n",
    "                titleSearch=f\"doSearch?fillQuickSearch=false&target=advanced&expand=dl&field1=AllField&text1={search}\"\n",
    "            case SearchWhere.Fulltext:\n",
    "                titleSearch=f\"doSearch?fillQuickSearch=false&target=advanced&expand=dl&field1=Fulltext&text1={search}\"\n",
    "        URL = f\"https://dl.acm.org/action/{titleSearch}&SeriesKeyAnd=imwut&startPage=\"\n",
    "        return URL\n",
    "\n",
    "    else:\n",
    "        print(f\"Library {library} not yet supported\")\n",
    "    return URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadACMBib (toOpen, driver):\n",
    "    driver.get(toOpen)#put here the adress of your page\n",
    "    # delay = 3 # seconds\n",
    "    \n",
    "    try: \n",
    "        # Only accept necessary cookies to resolve cookie popup\n",
    "        driver.find_element(By.ID, \"CybotCookiebotDialogBodyLevelButtonLevelOptinDeclineAll\").click()\n",
    "    except: \n",
    "        print(\"No cookie popup found, continuing.\")\n",
    "        pass\n",
    "\n",
    "    # Wait for the cookie dialog to disappear\n",
    "    WebDriverWait(driver, 10).until(EC.invisibility_of_element((By.ID, \"CybotCookiebotDialogBodyContent\")))\n",
    "\n",
    "    #iterate over middle navbar to see if query found paper results or only people\n",
    "    driver.find_element(by=By.CLASS_NAME, value=\"item-results__checkbox\").click()\n",
    "    time.sleep(5)\n",
    "    driver.find_element(by=By.CLASS_NAME, value=\"item-results__buttons.visible\").find_elements(by=By.XPATH, value=\".//*\")[0].click()\n",
    "    time.sleep(20)\n",
    "    \n",
    "    # Instead, click \"All Results\"\n",
    "    driver.find_element(by=By.ID, value=\"allResults\").click()\n",
    "    time.sleep(20)\n",
    "    driver.find_element(by=By.CLASS_NAME, value=\"downloadBtn\").click()\n",
    "    time.sleep(15)\n",
    "\n",
    "    # Wait for the popup and the new download button to be visible\n",
    "    export_div = WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"exportDownloadReady\"]/div[2]')))\n",
    "\n",
    "    # Find the 'a' element inside the located div, which contains our href of interest\n",
    "    download_now_button = export_div.find_element(By.XPATH, './/a[@class=\"btn blue searchCiteExport-popup__close pull-right\"]')\n",
    "\n",
    "    # Scroll into view and click the 'a' element\n",
    "    driver.execute_script(\"arguments[0].scrollIntoView();\", download_now_button)\n",
    "    download_now_button.click()\n",
    "    time.sleep(2)\n",
    "    \n",
    "def saveACMBib(keywords_list, dl_folder, searchWhere = SearchWhere.Anywhere):\n",
    "    driver = setupCrawler(dl_folder)\n",
    "    for keywords in keywords_list:\n",
    "        print(f\"Search for: {keywords}\")\n",
    "        ACM_URL = getURL(keywords, Library.ACM, searchWhere)\n",
    "        if DEBUG > 0: print(ACM_URL)\n",
    "        driver.get(ACM_URL)#put here the adress of your page\n",
    "        time.sleep(3)\n",
    "        navbar = driver.find_elements(by=By.CLASS_NAME, value=\"search-result__nav-container\")\n",
    "        navbar = navbar[0]\n",
    "        navelements = navbar.find_elements(by=By.XPATH, value=\".//*\")\n",
    "        foundResults = False\n",
    "        for nav_element in navelements:\n",
    "            if \"RESULTS\" in nav_element.text: \n",
    "                foundResults = True\n",
    "        if foundResults == False: \n",
    "            print(\"Only people in results - next keyword\")\n",
    "            continue\n",
    "        name = \"\"\n",
    "        for word in keywords:\n",
    "            name += f\"{word}\"\n",
    "        match searchWhere:\n",
    "            case SearchWhere.Title:\n",
    "                name += \"_TitleOnly\" \n",
    "            case SearchWhere.Abstract:\n",
    "                name += \"_AbstractOnly\" \n",
    "            case SearchWhere.TitleAbstract:\n",
    "                print(\"Stopping\")\n",
    "                break \n",
    "            case SearchWhere.Anywhere:\n",
    "                name += \"_Anywhere\"\n",
    "            case SearchWhere.Fulltext:\n",
    "                name += \"_Fulltext\"\n",
    "        if DEBUG > 0: driver.save_screenshot(f\"./acm-query-dl/acm_{name}.png\")\n",
    "        # get amount of results for for-loop\n",
    "\n",
    "        try:\n",
    "            results = driver.find_element(by=By.CLASS_NAME, value=\"result__count\")\n",
    "            results = results.text.split(\" \")[0]\n",
    "            if \",\" in results:\n",
    "                results = results.replace(\",\", \"\")\n",
    "            results = int(results)\n",
    "            if results >= 1000:\n",
    "                print(f'NOTE: More than 1000 search results for keyword, export citations will not be comprehensive: {keywords}')\n",
    "        except NoSuchElementException:\n",
    "            results = 0\n",
    "        \n",
    "        # Navigate to page\n",
    "        loadACMBib(ACM_URL, driver)\n",
    "        print('URL:', ACM_URL)\n",
    "        try:\n",
    "            source = os.getcwd() + '/acm-query-dl/acm.bib'\n",
    "            destination = os.getcwd() + '/acm-query-dl/' + f'/acm_{name.replace(\"*\",\"8\")}.bib'\n",
    "\n",
    "            shutil.copy(source, destination)  # Copy the file\n",
    "            os.remove(source)  # Remove the original file\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(\"Only 1 bib entry in that file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Search\n",
    "Input: CSV with columns: keywords, searchwhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start crawling Library.ACM\n",
      "Driver setup complete.\n",
      "Search for: ['external+dataset*']\n",
      "https://dl.acm.org/action/doSearch?fillQuickSearch=false&target=advanced&expand=dl&field1=Fulltext&text1=%22external+dataset*%22&SeriesKeyAnd=imwut&startPage=\n",
      "URL: https://dl.acm.org/action/doSearch?fillQuickSearch=false&target=advanced&expand=dl&field1=Fulltext&text1=%22external+dataset*%22&SeriesKeyAnd=imwut&startPage=\n",
      "Search for: ['existing+dataset*']\n",
      "https://dl.acm.org/action/doSearch?fillQuickSearch=false&target=advanced&expand=dl&field1=Fulltext&text1=%22existing+dataset*%22&SeriesKeyAnd=imwut&startPage=\n",
      "No cookie popup found, continuing.\n",
      "URL: https://dl.acm.org/action/doSearch?fillQuickSearch=false&target=advanced&expand=dl&field1=Fulltext&text1=%22existing+dataset*%22&SeriesKeyAnd=imwut&startPage=\n",
      "Search for: ['datasets+used']\n",
      "https://dl.acm.org/action/doSearch?fillQuickSearch=false&target=advanced&expand=dl&field1=Fulltext&text1=%22datasets+used%22&SeriesKeyAnd=imwut&startPage=\n",
      "No cookie popup found, continuing.\n",
      "URL: https://dl.acm.org/action/doSearch?fillQuickSearch=false&target=advanced&expand=dl&field1=Fulltext&text1=%22datasets+used%22&SeriesKeyAnd=imwut&startPage=\n"
     ]
    }
   ],
   "source": [
    "queries_df = pd.read_csv('./acm-queries.csv')\n",
    "\n",
    "searchtypes = queries_df['searchwhere'].unique()\n",
    "\n",
    "for searchtype in searchtypes:\n",
    "    temp_df = queries_df[queries_df['searchwhere'] == searchtype]\n",
    "    keywords = [[keyword] for keyword in temp_df['keywords']]\n",
    "    if searchtype == 'Title':\n",
    "        crawl(keywords, Library.ACM, SearchWhere.Title)\n",
    "    elif searchtype == 'Abstract':\n",
    "        crawl(keywords, Library.ACM, SearchWhere.Abstract)\n",
    "    elif searchtype == 'TitleAbstract':\n",
    "        break\n",
    "    elif searchtype == 'Anywhere':\n",
    "        crawl(keywords, Library.ACM, SearchWhere.Anywhere)\n",
    "    elif searchtype == 'Fulltext':\n",
    "        crawl(keywords, Library.ACM, SearchWhere.Fulltext)\n",
    "    else:\n",
    "        print('Invalid search term.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a loop here that takes in a csv of:\n",
    "keywords, where they are searched\n",
    "\n",
    "it coverts the keywords to lists based on where they are searched and searches all of them, creating .bib files\n",
    "\n",
    "it returns a list of keywords for which there were greater than 1000 results (need a script for this, can modify the original script that goes through pages)\n",
    "\n",
    "ready to run the next part of the notebook, that adds them to columns. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Append search queries to existing CSV\n",
    "Input: directory of bibtex files, csv on which to append the bibtex files.\n",
    "Output: csv with appended columns for each bib file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/audreyxychang/Documents/hci-lab/acm-query-dl/acm_classification+model8_Fulltext.bib\n",
      "Successfully parsed: acm_classification+model8_Fulltext.bib, writing to csv.\n",
      "/Users/audreyxychang/Documents/hci-lab/acm-query-dl/acm_training+set_Fulltext.bib\n",
      "Successfully parsed: acm_training+set_Fulltext.bib, writing to csv.\n",
      "/Users/audreyxychang/Documents/hci-lab/acm-query-dl/acm_pretrain8_Fulltext.bib\n",
      "Successfully parsed: acm_pretrain8_Fulltext.bib, writing to csv.\n",
      "/Users/audreyxychang/Documents/hci-lab/acm-query-dl/acm_existing+dataset8_Fulltext.bib\n",
      "Successfully parsed: acm_existing+dataset8_Fulltext.bib, writing to csv.\n",
      "/Users/audreyxychang/Documents/hci-lab/acm-query-dl/acm_LLM_AbstractOnly.bib\n",
      "Successfully parsed: acm_LLM_AbstractOnly.bib, writing to csv.\n",
      "/Users/audreyxychang/Documents/hci-lab/acm-query-dl/acm_neural+network_AbstractOnly.bib\n",
      "Successfully parsed: acm_neural+network_AbstractOnly.bib, writing to csv.\n",
      "/Users/audreyxychang/Documents/hci-lab/acm-query-dl/acm_review_AbstractOnly.bib\n",
      "Successfully parsed: acm_review_AbstractOnly.bib, writing to csv.\n",
      "/Users/audreyxychang/Documents/hci-lab/acm-query-dl/acm_gradient+boosting_Fulltext.bib\n",
      "Successfully parsed: acm_gradient+boosting_Fulltext.bib, writing to csv.\n",
      "/Users/audreyxychang/Documents/hci-lab/acm-query-dl/acm_dataset_TitleOnly.bib\n",
      "Successfully parsed: acm_dataset_TitleOnly.bib, writing to csv.\n",
      "/Users/audreyxychang/Documents/hci-lab/acm-query-dl/acm_external+dataset8_Fulltext.bib\n",
      "Successfully parsed: acm_external+dataset8_Fulltext.bib, writing to csv.\n",
      "/Users/audreyxychang/Documents/hci-lab/acm-query-dl/acm_pre-train8_Fulltext.bib\n",
      "Successfully parsed: acm_pre-train8_Fulltext.bib, writing to csv.\n",
      "/Users/audreyxychang/Documents/hci-lab/acm-query-dl/acm_we+train8_Fulltext.bib\n",
      "Successfully parsed: acm_we+train8_Fulltext.bib, writing to csv.\n",
      "/Users/audreyxychang/Documents/hci-lab/acm-query-dl/acm_we+collect8_AbstractOnly.bib\n",
      "Successfully parsed: acm_we+collect8_AbstractOnly.bib, writing to csv.\n",
      "/Users/audreyxychang/Documents/hci-lab/acm-query-dl/acm_hyperparameter+tuning_Fulltext.bib\n",
      "Successfully parsed: acm_hyperparameter+tuning_Fulltext.bib, writing to csv.\n",
      "/Users/audreyxychang/Documents/hci-lab/acm-query-dl/acm_regression+model8_Fulltext.bib\n",
      "Successfully parsed: acm_regression+model8_Fulltext.bib, writing to csv.\n",
      "/Users/audreyxychang/Documents/hci-lab/acm-query-dl/acm_machine+learning_AbstractOnly.bib\n",
      "Successfully parsed: acm_machine+learning_AbstractOnly.bib, writing to csv.\n",
      "/Users/audreyxychang/Documents/hci-lab/acm-query-dl/acm_survey_TitleOnly.bib\n",
      "Successfully parsed: acm_survey_TitleOnly.bib, writing to csv.\n",
      "/Users/audreyxychang/Documents/hci-lab/acm-query-dl/acm_datasets+used_Fulltext.bib\n",
      "Successfully parsed: acm_datasets+used_Fulltext.bib, writing to csv.\n",
      "/Users/audreyxychang/Documents/hci-lab/acm-query-dl/acm_LSTM_Fulltext.bib\n",
      "Successfully parsed: acm_LSTM_Fulltext.bib, writing to csv.\n",
      "/Users/audreyxychang/Documents/hci-lab/acm-query-dl/acm_decision+tree_Fulltext.bib\n",
      "Successfully parsed: acm_decision+tree_Fulltext.bib, writing to csv.\n",
      "/Users/audreyxychang/Documents/hci-lab/acm-query-dl/acm_publicly+available+data8_Fulltext.bib\n",
      "Successfully parsed: acm_publicly+available+data8_Fulltext.bib, writing to csv.\n",
      "/Users/audreyxychang/Documents/hci-lab/acm-query-dl/acm_benchmark+dataset8_Fulltext.bib\n",
      "Successfully parsed: acm_benchmark+dataset8_Fulltext.bib, writing to csv.\n",
      "/Users/audreyxychang/Documents/hci-lab/acm-query-dl/acm_SVM_Fulltext.bib\n",
      "Successfully parsed: acm_SVM_Fulltext.bib, writing to csv.\n",
      "/Users/audreyxychang/Documents/hci-lab/acm-query-dl/acm_validation+set_Fulltext.bib\n",
      "Successfully parsed: acm_validation+set_Fulltext.bib, writing to csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pybtex.database.input import bibtex\n",
    "from pybtex.database import parse_file as parser\n",
    "import time\n",
    "\n",
    "# Path to the dir\n",
    "directory_path = os.getcwd()+ '/acm-query-dl'\n",
    "csv_directory_path = os.getcwd() + '/'\n",
    "csv_filename = 'master_imwut.csv'\n",
    "\n",
    "# Open CSV as dataframe\n",
    "df = pd.read_csv(csv_directory_path + csv_filename)\n",
    "\n",
    "# Iterate through all files in the directory, which contains search queries\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.endswith('.bib'):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        print(file_path)\n",
    "\n",
    "        # Read the file with pybtex parser\n",
    "        parser = bibtex.Parser()\n",
    "        bib_data = parser.parse_file(file_path)\n",
    "        bib_dois = [key for (key,value) in bib_data.entries.items()]\n",
    "        print(f'Successfully parsed: {filename}, writing to csv.')\n",
    "\n",
    "        # Add column to df\n",
    "        colname = filename.split('.')[0]\n",
    "        df[colname] = df['id'].isin(bib_dois)\n",
    "    \n",
    "# Write the updated DataFrame back to a new CSV file\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "df.to_csv(csv_directory_path + 'query-modified-masters/' + csv_filename.split('.')[0] + f'_modified_{timestr}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6532684ccaeb1bcbbe852b7f75c67e6f1d55df7d386020fd37670376cbe3d2c9"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
