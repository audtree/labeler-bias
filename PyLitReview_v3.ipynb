{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements:\n",
    "\n",
    "- https://selenium-python.readthedocs.io/locating-elements.html\n",
    "- https://sites.google.com/a/chromium.org/chromedriver/downloads\n",
    "\n",
    "Note from Audrey: I did not use this script at all! It is simply Luke's Python scraping script pared down for ACM only. Instead of iterating through pages (which this script does), PyLitReview_v4 selects \"All Citations\" to download at once. Do not use this notebook to replicate Audrey's data collection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen, Request\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from html.parser import HTMLParser\n",
    "import tqdm\n",
    "import math\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException, ElementNotInteractableException\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "\n",
    "from pybtex.database.input import bibtex\n",
    "import pybtex.errors\n",
    "pybtex.errors.set_strict_mode(False)\n",
    "\n",
    "import itertools\n",
    "from itertools import permutations \n",
    "\n",
    "import os\n",
    "import time\n",
    "# import config\n",
    "from enum import Enum\n",
    "\n",
    "import bibtexparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0: No Screenshots\n",
    "# 1: One Screenshot for each query (recommended)\n",
    "# 2: Screenshots of different steps to find out why crawler might not work\n",
    "DEBUG = 2\n",
    "\n",
    "acm_maxpage = 39\n",
    "\n",
    "GLOBAL_ERROR_LIST = []\n",
    "urls = []\n",
    "\n",
    "# chrome_options = Options()\n",
    "# chrome_options.add_argument(\"--headless=new\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings for crawling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchWhere(Enum):\n",
    "        Title = 1\n",
    "        Abstract = 2\n",
    "        TitleAbstract = 3 #Keywords have to be in Title OR Abstract\n",
    "        Text = 4\n",
    "class Library(Enum):\n",
    "        IEEE = 1\n",
    "        ACM = 2\n",
    "        ScienceDirect = 3\n",
    "\n",
    "year_min = 2015 # Set to earliest year which should be crawled\n",
    "year_max = 2024 # Set to latest year whichh should be crawled\n",
    "\n",
    "# keywords = [['behavior change'], ['behaviour change']]\n",
    "keywords = [['dataset']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['dataset']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup for crawler\n",
    "\n",
    "### function to crawl: crawl(keywords, LIBRARY, titlesearch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setupCrawler(dl_folder):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('window-size=1920,1080')\n",
    "    dl = \"\"\n",
    "    p = {\"download.default_directory\": dl}\n",
    "    options.add_experimental_option(\"prefs\", p)\n",
    "    op = webdriver.ChromeOptions()\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    print(\"Driver setup complete.\")\n",
    "    return driver\n",
    "\n",
    "def crawl(keywords_list, library, searchWhere):\n",
    "    print(f\"Start crawling {library}\")\n",
    "    if library == Library.ACM:\n",
    "        keywords = [[item.replace(\" \", \"+\") for item in keywords] for keywords in keywords_list]\n",
    "        saveACMBib(keywords, Library.ACM, searchWhere)\n",
    "    else:\n",
    "        print(f\"Library {library} not yet supported\")\n",
    "\n",
    "def getURL(keywords, library, searchWhere, concatentation=\"AND\"):\n",
    "    URL = \"\"\n",
    "    search = \"\"\n",
    "    if library == Library.ACM:\n",
    "        titleSearch = \"doSearch?AllField=\"\n",
    "        for i, keyword in enumerate(keywords):\n",
    "            search += f\"%22{keyword}%22\"\n",
    "            if (i < len(keywords)-1):\n",
    "                search += f\"+{concatentation}+\"\n",
    "        match searchWhere:\n",
    "            case SearchWhere.Title:\n",
    "                print(\"Searching ACM for title only\")\n",
    "                titleSearch = f\"doSearch?fillQuickSearch=false&expand=dl&field1=Title&text1={search}\"\n",
    "            case SearchWhere.Abstract:\n",
    "                print(\"Searching ACM for abstract only\")\n",
    "                titleSearch = f\"doSearch?fillQuickSearch=false&expand=dl&field1=Abstract&text1={search}\"\n",
    "            case SearchWhere.TitleAbstract:\n",
    "                print(\"ACM does not support searching for keywords in Title OR Abstract. Please use Title and Abstract search seperately.\")\n",
    "            case SearchWhere.Text | _:\n",
    "                print(\"Quicksearching ACM\")\n",
    "        URL = f\"https://dl.acm.org/action/{titleSearch}&SeriesKeyAnd=imwut&startPage=\"\n",
    "        return URL\n",
    "\n",
    "    else:\n",
    "        print(f\"Library {library} not yet supported\")\n",
    "    return URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadACMBib (toOpen, driver):\n",
    "    driver.get(toOpen)#put here the adress of your page\n",
    "    # delay = 3 # seconds\n",
    "    \n",
    "    try: \n",
    "        # Only accept necessary cookies to resolve cookie popup\n",
    "        driver.find_element(By.ID, \"CybotCookiebotDialogBodyLevelButtonLevelOptinDeclineAll\").click()\n",
    "    except: \n",
    "        print(\"No cookie popup found, continuing.\")\n",
    "\n",
    "    # Wait for the cookie dialog to disappear\n",
    "    WebDriverWait(driver, 10).until(EC.invisibility_of_element((By.ID, \"CybotCookiebotDialogBodyContent\")))\n",
    "\n",
    "    #iterate over middle navbar to see if query found paper results or only people\n",
    "    driver.find_element(by=By.CLASS_NAME, value=\"item-results__checkbox\").click()\n",
    "    time.sleep(5)\n",
    "    driver.find_element(by=By.CLASS_NAME, value=\"item-results__buttons.visible\").find_elements(by=By.XPATH, value=\".//*\")[0].click()\n",
    "    time.sleep(20)\n",
    "    driver.find_element(by=By.CLASS_NAME, value=\"rlist--inline.separator\").find_elements(by=By.XPATH, value=\".//*\")[0].click()\n",
    "    time.sleep(20)\n",
    "    \n",
    "def saveACMBib(keywords_list, dl_folder, searchWhere = SearchWhere.Text):\n",
    "    driver = setupCrawler(dl_folder)\n",
    "    for keywords in keywords_list:\n",
    "        print(f\"Search for: {keywords}\")\n",
    "        ACM_URL = getURL(keywords, Library.ACM, searchWhere)\n",
    "        if DEBUG > 0: print(ACM_URL)\n",
    "        driver.get(ACM_URL)#put here the adress of your page\n",
    "        time.sleep(3)\n",
    "        navbar = driver.find_elements(by=By.CLASS_NAME, value=\"search-result__nav-container\")\n",
    "        navbar = navbar[0]\n",
    "        navelements = navbar.find_elements(by=By.XPATH, value=\".//*\")\n",
    "        foundResults = False\n",
    "        for nav_element in navelements:\n",
    "            if \"RESULTS\" in nav_element.text: \n",
    "                foundResults = True\n",
    "        if foundResults == False: \n",
    "            print(\"Only people in results - next keyword\")\n",
    "            continue\n",
    "        name = \"\"\n",
    "        for word in keywords:\n",
    "            name += f\"{word}\"\n",
    "        match searchWhere:\n",
    "            case SearchWhere.Title:\n",
    "                name += \"_TitleOnly\" \n",
    "            case SearchWhere.Abstract:\n",
    "                name += \"_AbstractOnly\" \n",
    "            case SearchWhere.TitleAbstract:\n",
    "                print(\"Stopping\")\n",
    "                break \n",
    "                # name += \"_TitleAbstract\"\n",
    "            case SearchWhere.Text | _:\n",
    "                name += \"\"  \n",
    "        if DEBUG > 0: driver.save_screenshot(f\"./acm_{name}.png\")\n",
    "        # get amount of results for for-loop\n",
    "\n",
    "        try:\n",
    "            results = driver.find_element(by=By.CLASS_NAME, value=\"result__count\")\n",
    "            results = results.text.split(\" \")[0]\n",
    "            if \",\" in results:\n",
    "                results = results.replace(\",\", \"\")\n",
    "            results = int(results)\n",
    "        except NoSuchElementException:\n",
    "            results = 0\n",
    "        r = np.min([math.ceil(results / 50), acm_maxpage])\n",
    "        # Loop through all pages and save resulting bib files\n",
    "        for i in tqdm.tqdm(range(r)):\n",
    "            # toOpen = ACM_URL + str(i) \n",
    "            toOpen = ACM_URL + str(i) + '&pageSize=50'\n",
    "            driver = setupCrawler(dl_folder) # I think this is unnecessary - if something breaks with ACM try uncommenting this line first\n",
    "            loadACMBib(toOpen, driver)\n",
    "            print(toOpen)\n",
    "            try:\n",
    "                os.rename('./acm/acm.bib', f'./acm/acm_{name.replace(\"*\",\"\")}_page{i}.bib')\n",
    "            except FileNotFoundError:\n",
    "                print(\"Only 1 bib entry in that file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No results for now\n",
    "# crawl(keywords, Library.ACM, SearchWhere.Title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawl(keywords, Library.ACM, SearchWhere.Text)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6532684ccaeb1bcbbe852b7f75c67e6f1d55df7d386020fd37670376cbe3d2c9"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
